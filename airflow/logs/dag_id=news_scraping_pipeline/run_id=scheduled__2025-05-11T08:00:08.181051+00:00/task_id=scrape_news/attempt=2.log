[2025-05-12T08:05:58.337+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: news_scraping_pipeline.scrape_news scheduled__2025-05-11T08:00:08.181051+00:00 [queued]>
[2025-05-12T08:05:58.363+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: news_scraping_pipeline.scrape_news scheduled__2025-05-11T08:00:08.181051+00:00 [queued]>
[2025-05-12T08:05:58.364+0000] {taskinstance.py:1359} INFO - Starting attempt 2 of 2
[2025-05-12T08:05:58.407+0000] {taskinstance.py:1380} INFO - Executing <Task(DockerOperator): scrape_news> on 2025-05-11 08:00:08.181051+00:00
[2025-05-12T08:05:58.423+0000] {standard_task_runner.py:57} INFO - Started process 53 to run task
[2025-05-12T08:05:58.449+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'news_scraping_pipeline', 'scrape_news', 'scheduled__2025-05-11T08:00:08.181051+00:00', '--job-id', '33', '--raw', '--subdir', 'DAGS_FOLDER/news_scraping_dag.py', '--cfg-path', '/tmp/tmpp1ie12ee']
[2025-05-12T08:05:58.456+0000] {standard_task_runner.py:85} INFO - Job 33: Subtask scrape_news
[2025-05-12T08:05:58.498+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 33 for task scrape_news (invalid interpolation syntax in 'dag_id={{ ti.dag_id }}/run_id={{ ti.run_id }}/task_id={{ ti.task_id }}/{% if ti.map_index >= 0 %}map_index={{ ti.map_index }}/{% endif %}attempt={{ try_number }}.log' at position 72; 53)
[2025-05-12T08:05:58.541+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-05-12T08:05:58.607+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
