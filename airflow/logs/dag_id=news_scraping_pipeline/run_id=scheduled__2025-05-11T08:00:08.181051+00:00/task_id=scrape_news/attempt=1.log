[2025-05-12T08:00:16.678+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: news_scraping_pipeline.scrape_news scheduled__2025-05-11T08:00:08.181051+00:00 [queued]>
[2025-05-12T08:00:16.694+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: news_scraping_pipeline.scrape_news scheduled__2025-05-11T08:00:08.181051+00:00 [queued]>
[2025-05-12T08:00:16.695+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 2
[2025-05-12T08:00:16.713+0000] {taskinstance.py:1380} INFO - Executing <Task(DockerOperator): scrape_news> on 2025-05-11 08:00:08.181051+00:00
[2025-05-12T08:00:16.717+0000] {standard_task_runner.py:57} INFO - Started process 54 to run task
[2025-05-12T08:00:16.723+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'news_scraping_pipeline', 'scrape_news', 'scheduled__2025-05-11T08:00:08.181051+00:00', '--job-id', '15', '--raw', '--subdir', 'DAGS_FOLDER/news_scraping_dag.py', '--cfg-path', '/tmp/tmpznpt4ghs']
[2025-05-12T08:00:16.726+0000] {standard_task_runner.py:85} INFO - Job 15: Subtask scrape_news
[2025-05-12T08:00:16.742+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 15 for task scrape_news (invalid interpolation syntax in 'dag_id={{ ti.dag_id }}/run_id={{ ti.run_id }}/task_id={{ ti.task_id }}/{% if ti.map_index >= 0 %}map_index={{ ti.map_index }}/{% endif %}attempt={{ try_number }}.log' at position 72; 54)
[2025-05-12T08:00:16.753+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-05-12T08:00:16.782+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
