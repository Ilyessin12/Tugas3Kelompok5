[2025-05-12T08:00:24.077+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: news_scraping_pipeline.scrape_news manual__2025-05-12T08:00:22.522379+00:00 [queued]>
[2025-05-12T08:00:24.095+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: news_scraping_pipeline.scrape_news manual__2025-05-12T08:00:22.522379+00:00 [queued]>
[2025-05-12T08:00:24.096+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 2
[2025-05-12T08:00:24.122+0000] {taskinstance.py:1380} INFO - Executing <Task(DockerOperator): scrape_news> on 2025-05-12 08:00:22.522379+00:00
[2025-05-12T08:00:24.131+0000] {standard_task_runner.py:57} INFO - Started process 64 to run task
[2025-05-12T08:00:24.136+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'news_scraping_pipeline', 'scrape_news', 'manual__2025-05-12T08:00:22.522379+00:00', '--job-id', '20', '--raw', '--subdir', 'DAGS_FOLDER/news_scraping_dag.py', '--cfg-path', '/tmp/tmpmj86e_36']
[2025-05-12T08:00:24.139+0000] {standard_task_runner.py:85} INFO - Job 20: Subtask scrape_news
[2025-05-12T08:00:24.161+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 20 for task scrape_news (invalid interpolation syntax in 'dag_id={{ ti.dag_id }}/run_id={{ ti.run_id }}/task_id={{ ti.task_id }}/{% if ti.map_index >= 0 %}map_index={{ ti.map_index }}/{% endif %}attempt={{ try_number }}.log' at position 72; 64)
[2025-05-12T08:00:24.193+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-05-12T08:00:24.227+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
